{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Pattern Recognition - Solution 4: Nonparametric methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Y_I858rzcTU"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQySghbok58J"
   },
   "source": [
    "## $\\star$ Part 1: K-nearest neighbors\n",
    "\n",
    "Load the data from dataset.npz and split it evenly into a training set\n",
    "and a test set. \n",
    "\n",
    "Each sample consists of a point in 2D and a class label\n",
    "$\\{1,2,3\\}$.\n",
    "\n",
    "For each point in the test set, predict its label by a k-nearest\n",
    "neighbor classifier \u201ctrained\u201d using the training set. \n",
    "\n",
    "Compute the **average classification error** using the true labels of the test set. \n",
    "\n",
    "Visualize the training and test points with their respective label, as well as the classifier's decision boundary using a **contour plot** (see example at the bottom).\n",
    "\n",
    "Repeat for different values of k.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SelA7fkr1Mr8",
    "outputId": "96016d7f-65fa-48ab-8240-7c6bc43a9b59"
   },
   "outputs": [],
   "source": [
    "\n",
    "# load the data and split it into train and test sets\n",
    "# START TODO ################\n",
    "raise NotImplementedError\n",
    "# END TODO ################\n",
    "\n",
    "# check that the data and the split shapes are correct\n",
    "# START TODO ################\n",
    "# here we check whether the data is equally split\n",
    "# between test and train sets\n",
    "raise NotImplementedError\n",
    "# END TODO ################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and plot the k-nearest neighbors classifier\n",
    "def plot_k_neighbors(k, data_train, data_test, classifier=KNeighborsClassifier):\n",
    "    \"\"\"\n",
    "    k: Number of neighbors to use\n",
    "    data_train: subsection of the dataset that is to be used for training\n",
    "    data_test subsection of the dataset that is to be used for testing\n",
    "\n",
    "    For more information on the KNeighborsClassifier see:\n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "    \"\"\"\n",
    "    # START TODO ################\n",
    "    raise NotImplementedError\n",
    "    # END TODO ################\n",
    "\n",
    "plot_k_neighbors(2, data_train, data_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1MYF6jXZnCr3",
    "outputId": "e145a019-6a44-4d69-b6af-809b291f3caf",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# test for different values of K\n",
    "for k in range(1, 8):\n",
    "    plot_k_neighbors(k, data_train, data_test)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMUjMh7Fk4Ua"
   },
   "source": [
    "## $\\star$ Part 2: Data splits and hyperparameters\n",
    "\n",
    "Study how different dataset splits and values of k affect classification results.\n",
    "\n",
    "### Part 2.1\n",
    "Train and display k-nearest-neighbor models for different *equally sized* splits of the set into training and test set. Use fixed k=4.\n",
    "\n",
    "Is the classification error always the same?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "09eHqvKNt-G9",
    "outputId": "e2999200-1844-4473-eb63-3e2adde96a0e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fix K=4 and check how different random data splits change the result\n",
    "# START TODO ################\n",
    "raise NotImplementedError\n",
    "# END TODO ################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.2\n",
    "Train and display k-nearest-neighbor models for differently sized train/test splits. Use fixed k=4.\n",
    "\n",
    "How does the classification error change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix K=4 and check how different training set sizes change the result\n",
    "# START TODO ################\n",
    "raise NotImplementedError\n",
    "# END TODO ################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.3\n",
    "Compute and plot the classification error for different values of k, averaged over multiple *equally sized* splits.\n",
    "\n",
    "How is the mean error affected by k?\n",
    "\n",
    "What you should observe is an effect illustrating the so-called bias-variance tradeoff and will be discussed in more detail in the next class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sbQZE-rlud-K"
   },
   "outputs": [],
   "source": [
    "def get_classification_errors(data, k, n_trials):\n",
    "    '''\n",
    "    Run n_trials experiments with different (equally sized) train-test splits.\n",
    "    Return a list of classification errors for each run.\n",
    "    '''\n",
    "    errors = []\n",
    "    # START TODO ################\n",
    "    raise NotImplementedError\n",
    "    # END TODO ################\n",
    "    return errors\n",
    "\n",
    "ks = range(1, 50)\n",
    "n_trials = 100\n",
    "errors = []\n",
    "\n",
    "# compute the mean classification error for each value of K and plot it as a function of K\n",
    "# START TODO ################\n",
    "raise NotImplementedError\n",
    "# END TODO ################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\star\\star\\star$ Bonus part: Custom KNN estimator\n",
    "\n",
    "Implement the same functionality as `KNeighborsClassifier`.\n",
    "\n",
    "For this, you can create a new class `SimpleKNeighborsClassifier` and implement the methods `__init__`, `fit` and `predict`.\n",
    "\n",
    "The general idea is to:\n",
    "\n",
    "* Convert the class labels from {1, 2, 3} to {0, 1, 2}\n",
    "* Convert the integer classes to [onehot vectors](https://en.wikipedia.org/wiki/One-hot#Machine_learning_and_statistics).\n",
    "* Compute the distance from all test points to all training points.\n",
    "* For each test point, sort the training points by ascending distance, only keep the top k datapoints, average the class probabilities of those k points and predict the class with the highest probability.\n",
    "* Convert the classes back from {0, 1, 2} to {1, 2, 3}\n",
    "\n",
    "Check if your estimator produces the same results as the estimator from `sklearn`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_onehot_matrix(targets:np.ndarray, num_classes:int):\n",
    "    # START TODO ################\n",
    "    # targets shape (n_datapoints)\n",
    "    # we want a matrix of shape (n_datapoints, num_classes) s. t.\n",
    "    # entry[n, cls] = 1 if targets[n] == cls else 0\n",
    "    raise NotImplementedError\n",
    "    # END TODO ################\n",
    "\n",
    "# test the onehot function with mockup targets\n",
    "example_targets = np.array([1.,3.,0.])\n",
    "num_classes = 5\n",
    "example_onehot = get_onehot_matrix(\n",
    "    example_targets, num_classes)\n",
    "\n",
    "print(f\"Given targets {example_targets} and {num_classes} classes,\"\n",
    "      \"\\nresulting onehot matrix is:\\n\"\n",
    "      f\"{example_onehot}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class SimpleKNeighborsClassifier:\n",
    "    # START TODO ################\n",
    "    raise NotImplementedError\n",
    "    # END TODO ################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---------- Custom implementation:\")\n",
    "plot_k_neighbors(4, data_train, data_test,\n",
    "                 classifier=SimpleKNeighborsClassifier)\n",
    "print()\n",
    "print(\"---------- Implementation from sklearn:\")\n",
    "plot_k_neighbors(4, data_train, data_test,\n",
    "                 classifier=KNeighborsClassifier)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hints\n",
    "\n",
    "### Example output for Part 1\n",
    "\n",
    "![example output](ex4_example_output.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating contour plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating contour plots\n",
    "\n",
    "# create custom colormaps\n",
    "cmap_light = ListedColormap(['#ff9f2f', '#5fff5f', '#7f7fff'])\n",
    "cmap_bold = ListedColormap(['#af4f00', '#0faf0f', '#2f2fcf'])\n",
    "\n",
    "# create some example data in sine form\n",
    "data = np.random.uniform(-1.5, 1.5, size=(500,2))\n",
    "data[:, 0] *= 3.14\n",
    "x, y = data[:, 0], data[:, 1]\n",
    "\n",
    "# set data classes with sine as border\n",
    "y_border = np.sin(data[:, 0])\n",
    "classes = (y > y_border).astype(float)\n",
    "\n",
    "# plot points colored by classes with the created colormap\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(data[:, 0], data[:, 1], c=classes, cmap=cmap_bold)\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a meshgrid depending on data range\n",
    "bordersize = .1\n",
    "x_min, x_max = x.min() - bordersize, x.max() + bordersize\n",
    "y_min, y_max = y.min() - bordersize, y.max() + bordersize\n",
    "\n",
    "grid_size = .02\n",
    "xrange = np.arange(x_min, x_max, grid_size)\n",
    "yrange = np.arange(y_min, y_max, grid_size)\n",
    "print(f\"{xrange.shape=}, {yrange.shape=}\")\n",
    "\n",
    "xx, yy = np.meshgrid(xrange, yrange)\n",
    "print(f\"{xx.shape=}, {yy.shape=}\")\n",
    "\n",
    "# now xx maps from pixel position i, j to position x in the data\n",
    "\n",
    "# classify each point in the meshgrid\n",
    "zz = (yy > np.sin(xx)).astype(float)\n",
    "\n",
    "# plot the contour of the true class distribution and the samples\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.contourf(xx, yy, zz, cmap=cmap_light)\n",
    "plt.scatter(data[:, 0], data[:, 1], c=classes, cmap=cmap_bold)\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling data shapes\n",
    "\n",
    "The sklearn predictor expects input of shape `(datapoints, features)`. To input your meshgrid, use `np.reshape` to flatten both `xx` and `yy` and then use `np.stack` to stack them in the last axis. Finally `reshape` the predictor's output back to your meshgrid shape and you can plot the contour.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inplace modification in numpy\n",
    "\n",
    "Be mindful of whether you are working with *copies* or *views* of your data.\n",
    "\n",
    "Comparison of copying and inplace modification in numpy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"---------- Numpy ----------\")\n",
    "a = np.array([5])\n",
    "b = a\n",
    "b = b - 1\n",
    "print(f\"Copy:    {a} {b}\")\n",
    "\n",
    "a = np.array([5])\n",
    "b = a\n",
    "b -= 1\n",
    "print(f\"Inplace: {a} {b}\")\n",
    "\n",
    "print(f\"---------- Python ----------\")\n",
    "a = 5\n",
    "b = a\n",
    "b = b - 1\n",
    "print(f\"Copy:    {a} {b}\")\n",
    "\n",
    "a = 5\n",
    "b = a\n",
    "b -= 1\n",
    "print(f\"Inplace: {a} {b}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ex4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}