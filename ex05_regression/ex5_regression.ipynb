{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Pattern Recognition Exercise 5: Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WduJB1nIcLu6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge, BayesianRidge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "# get the default color cycle from matplotlib\n",
    "color_cycle = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "def get_color(i):\n",
    "    return color_cycle[i % len(color_cycle)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Scg1A6QvjCrr"
   },
   "source": [
    "## $\\star$ Part 1: Linear Regression with polynomials\n",
    "\n",
    "### Part 1.1: Load and plot data\n",
    "\n",
    "Load the points from `regression.npz`.\n",
    "\n",
    "The data contains $N$ datapoints and 2 columns with column 0 as x-axis (inputs $\\mathbf{x}$) and feature 1 as y-axis (targets $\\mathbf{t}$). Scatterplot the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5kWn96nQnTfr"
   },
   "outputs": [],
   "source": [
    "# START TODO ################\n",
    "# Load the data and print the shape\n",
    "raise NotImplementedError\n",
    "# END TODO ################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "XoaUsc7xkTL_",
    "outputId": "5234abce-3875-410b-aaac-f7914cce6235"
   },
   "outputs": [],
   "source": [
    "# START TODO ################\n",
    "# Scatterplot the data.\n",
    "# As usual feature 0 is the x-axis and feature 1 is the y-axis.\n",
    "raise NotImplementedError\n",
    "# END TODO ################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.2: Implementation\n",
    "\n",
    "Now the goal is to estimate the weight parameters using $m^{\\textrm{th}}$ order polynomials as basis functions using the ML Estimator.\n",
    "\n",
    "Implement function `get_poly_features` - Get the polynomial features given inputs $\\mathbf{x}$ and polynomial degree $m$. Compute the $M = m + 1$ basis functions with $\\phi_m(x_n) = x_n ^ m $ for $m = \\{0, ..., M\\}$ and $n = \\{0, ..., N-1\\}$. Return a matrix $\\Phi$ of shape $(N, M+1)$.\n",
    "\n",
    "Implement a predictor class with the following functions:\n",
    "\n",
    "1. `fit` - Estimate and save the weight parameters using the ML estimator. Hint: `np.linalg.inv` can inverse matrices.\n",
    "1. `predict` - Predict targets given inputs.\n",
    "\n",
    "Then:\n",
    "\n",
    "1. Create the predictor, fit the given data.\n",
    "1. Predict the targets for the given $\\mathbf{x}$, print the mean squared error (MSE) and scatterplot the predictions.\n",
    "1. Predict the targets for a suitable range of inputs and plot them to view the whole polynomial.\n",
    "\n",
    "See how the outcome changes as you change $m$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example output:\n",
    "\n",
    "![ex5_example_output_01.png](ex5_example_output_01.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kieaAHikq4AJ"
   },
   "outputs": [],
   "source": [
    "# START TODO ################\n",
    "# note the shapes:\n",
    "# x: inputs of shape (n_datapoints)\n",
    "# t: targets of shape (n_datapoints)\n",
    "# m: degree of the polynomial\n",
    "# See implementation text above for hints on how to build the predictor.\n",
    "raise NotImplementedError\n",
    "# END TODO ################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8sctdvrkthHB"
   },
   "outputs": [],
   "source": [
    "# START TODO ################\n",
    "# Build a function: Given data, labels and degree of the polynomial\n",
    "# create and fit the predictor, predict the data, print the MSE and plot\n",
    "# the polynomials and data.\n",
    "# Run the function on a suitable set of inputs.\n",
    "# If your plot gets too zoomed out, use plt.ylim([ymin, ymax]) with suitable\n",
    "# limits to constrain the y-axis.\n",
    "raise NotImplementedError\n",
    "# END TODO ################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxXQgLZZkTnm"
   },
   "source": [
    "## $\\star$ $\\star$ Part 2: Ridge regression\n",
    "\n",
    "### Part 2.1: Ridge Regression using sklearn\n",
    "\n",
    "Estimate the weights using the MAP estimator with the Gaussian prior\n",
    "from this class (Ridge Regression). Use class `Ridge` from sklearn as the predictor.\n",
    "\n",
    "Note the difference between Regularized Least Squares [1] used by sklearn's `Ridge` class with parameter $\\lambda$ and Bayesian Linear Regression with a zero-mean isotropic Gaussian prior [2] as used in the slides with parameters $\\alpha$ and $\\beta$. The resulting weights and the mean predictions are equivalent for $\\lambda = \\alpha\\ /\\ \\beta$. Parameter $\\beta$ is a prior for the precision of the prediction.\n",
    "\n",
    "Create the function `reg_ridge_sci` with parameters `x, y, t, alpha=0.001, beta=10` and create the predictor with `lambda = alpha / beta` and `Ridge(alpha=lambd, solver=\"svd\", fit_intercept=False)`. We can set `fit_intercept=False` since the polynomial with degree 0 will act as the intercept.\n",
    "\n",
    "To use this predictor, **either** reuse the function `get_poly_features` from above to create the polynomial input features for the Ridge model **or** create a pipeline: `make_pipeline(PolynomialFeatures(degree=m), Ridge(...))`. Note that in the second case you will need to expand the inputs `expa_x = x[:, None]` since sklearn expects inputs of shape `(n_datapoints, d_features)` even if `d_features == 1`.\n",
    "\n",
    "- Print the MSE.\n",
    "- Plot the true labels and the predictions. \n",
    "- Create a suitable input range and predict its targets to plot the polynomial line.\n",
    "- **Bonus**: Compute the standard deviation for a plot in the style of slide 15. Compute the inverse posterior covariance $S_N$ given the data (slide 11). Now, compute the prediction variance for the input range (eqn. 3, slide 14). This can be tricky in numpy. Given matrices `A` and `B` both of shape `(N, D)`, to get the `N` dot products you can use `np.einsum(\"nd,nd->n\", A, B)`. Alternatively, use slow for-loops. Compute standard deviation as square root of variance. Use `plt.fill_between` and set the transparency `alpha=0.2` to plot the standard deviation. Use `plt.ylim` to constrain the plot within `np.min(t) - 1, np.max(t) + 1`.\n",
    "\n",
    "See what happens as you use fewer and fewer points to estimate the weights.\n",
    "\n",
    "Also see what happens as you change the hyperparameters $\\alpha$, $\\beta$ and the order of the\n",
    "polynomial $m$. When must it be large, when can it be chosen smaller?\n",
    "\n",
    "- [1] Christopher M. Bishop, Pattern Recognition and Machine Learning, p. 144, eqn. 3.27\n",
    "- [2] Christopher M. Bishop, Pattern Recognition and Machine Learning, p. 153, eqn. 3.55\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example output for few datapoints:\n",
    "\n",
    "![ex5_example_output_02.png](ex5_example_output_02.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START TODO ################\n",
    "# Build the reg_ridge_sci function as described above.\n",
    "raise NotImplementedError\n",
    "# END TODO ################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Varying polynomial degree $m$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START TODO ################\n",
    "# Run the regression multiple times with varying polynomial degree m.\n",
    "# What do you observe?\n",
    "raise NotImplementedError\n",
    "# END TODO ################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Varying dataset size $N$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START TODO ################\n",
    "# Fix m=5 and run the regression multiple times,\n",
    "# this time varying the amount of data.\n",
    "# What do you observe?\n",
    "raise NotImplementedError\n",
    "# END TODO ################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Varying $\\alpha$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START TODO ################\n",
    "# Vary alpha. What do you observe?\n",
    "raise NotImplementedError\n",
    "# END TODO ################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Varying $\\beta$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START TODO ################\n",
    "# Vary beta. What do you observe?\n",
    "raise NotImplementedError\n",
    "# END TODO ################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixing $\\lambda = \\alpha\\ /\\ \\beta$ and varying their scale.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START TODO ################\n",
    "# Multiply both alpha and beta by the same varying constant.\n",
    "# What do you observe?\n",
    "raise NotImplementedError\n",
    "# END TODO ################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.2: Manual Ridge Regression\n",
    "\n",
    "Create the class `PolyRidgeRegression` to replace sklearn's `Ridge`, see slides \"Bayesian Regression\" and \"Posterior distribution\" for the formulae.\n",
    "\n",
    "Create a plot as in 2.2 and verify that the results match.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START TODO ################\n",
    "# class PolyRidgeRegression:\n",
    "raise NotImplementedError\n",
    "# END TODO ################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JvqtPY_FHSz1"
   },
   "outputs": [],
   "source": [
    "# START TODO ################\n",
    "# def reg_ridge_own(x, t, m, alpha=0.001, beta=10):\n",
    "raise NotImplementedError\n",
    "# END TODO ################\n",
    "\n",
    "# here we verify that the two functions produce the same plot and MSE\n",
    "reg_ridge_own(x, t, 5, alpha=0.001, beta=10)\n",
    "reg_ridge_sci(x, t, 5, alpha=0.001, beta=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TY3oSNXUkyIF"
   },
   "source": [
    "## $\\star \\star \\star$ Part 3: Hyperparameter optimization with EM\n",
    "\n",
    "Find the optimum hyperparameters using the evidence approximation. \n",
    "It is not that hard. You know EM and you know how to\n",
    "estimate the weight parameters. Just combine these two. \n",
    "\n",
    "See slide 24 and Bishop 3.5.1 pages 166-169, especially eqn. 3.95.\n",
    "\n",
    "Start the optimization with $\\alpha_0=0.001, \\beta_0=10$.\n",
    "Create two plots using function `reg_ridge_sci` from question 2, one with the old hyperparameters and one with the optimized hyperparameters. The MSE should be lower in the second case because you have found better hyper-parameters.\n",
    "\n",
    "Verify your results by fitting a `BayesianRidge` predictor from sklearn. The parameters are named differently: Create it as `BayesianRidge(lambda_init=alpha0, alpha_init=beta0, fit_intercept=False, verbose=True)`, fit and extract the results as `alpha = br.lambda_` and `beta = br.alpha_`. You should receive the same hyperparameters as in your own evidence approximation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 656
    },
    "id": "S3YdjFiFlnwR",
    "outputId": "3b3e99f5-bc16-4b85-b271-14ebb03ae714"
   },
   "outputs": [],
   "source": [
    "# START TODO ################\n",
    "# class EvidenceApproximator:\n",
    "raise NotImplementedError\n",
    "# END TODO ################\n",
    "\n",
    "m = 5\n",
    "alpha = 0.001\n",
    "beta = 10\n",
    "ea = EvidenceApproximator(m, alpha0=alpha, beta0=beta, iter_steps=20)\n",
    "new_alpha, new_beta = ea.run_em(x, t)\n",
    "\n",
    "# plot results\n",
    "reg_ridge_sci(x, t, m, alpha=alpha, beta=beta)\n",
    "reg_ridge_sci(x, t, m, alpha=new_alpha, beta=new_beta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j_W6hbrfRi6P"
   },
   "outputs": [],
   "source": [
    "# START TODO ################\n",
    "# Run BayesianRidge from sklearn and compare it to your own EvidenceApproximator\n",
    "# def reg_ridge_bayes(x, t, m, alpha0, beta0):\n",
    "raise NotImplementedError\n",
    "# END TODO ################\n",
    "\n",
    "reg_ridge_bayes(x, t, 5, alpha, beta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ex5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}